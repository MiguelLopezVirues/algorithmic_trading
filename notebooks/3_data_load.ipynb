{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.support.data_extraction import TickersFetcher\n",
    "from src.support.data_transformation import TickerExtender, TechnicalIndicators\n",
    "from src.support.data_load import MongoDBHandler\n",
    "\n",
    "from src.support.utils import load_tickers_to_include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to show the process followed to load data to MongoDB.\n",
    "\n",
    "## 1.1 Database choice\n",
    "For this project, the database choice is MongoDB. It is chosen above any other relational databases because, as the project is still in prototyping phase, it makes more sense due to the flexibility NoSQL provides. Among other NoSQL providers, MongoDB Atlas is chosen for its free tier and easy API integration.\n",
    "\n",
    "## 1.2 Data subject to upload\n",
    "There are various data that could be uploaded to the database by the nature of this project:\n",
    "1. Transformed data. Typical ETL applies here.\n",
    "2. Predictions data. For predictions to be tracked and also used by other services.\n",
    "3. Trade oprations data. For monitoring of the trading activity.\n",
    "\n",
    "In the case 1. Transformed data, there is a disclaimer; although the load part of the ETL is operational, it is in practice deactivated and not used due to the 0.5 GB storage limit present in MongoDB Atlas' free tier. Actually, data load for this is not even necessary, except for dataset versioning purposes for which a feature store (out of scope for this project) would be more appropriate, as API availability and the speed of transformation guarantee that at any time the data will be avaible in 20 seconds by executing the ETL code. Given that data would be used for prediction just once a week, or maximum once a day, the costs of storing big gigabytes of data outweight its benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data load process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load is handled by `src/support/data_load.py`, which is where the MongoDBHandler class stores the methods for uploading data to MongoDB.\n",
    "\n",
    "The scripts that exploit these methods are in:\n",
    "- `src/data_etl_pipeline.py`. The load code is commented out as explained above.\n",
    "- `src/deployment/predict.py`. Loads the predictions output by the model into the 'stocks_predictions' collection.\n",
    "- `src/trading/trading.py`. Interacts with the database to monitor trades in the 'trades' collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithmic_trading",
   "language": "python",
   "name": "algorithmic_trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
